---
title:  "GPT1: Improving Language Understandingby Generative Pre-Training review"
excerpt: "ë§¥ë½ê³¼ í•¨ê»˜ ìì„¸íˆ ì‚´í´ë³´ëŠ” GPT1 ë…¼ë¬¸ ë¦¬ë·°/ì„¤ëª…"
toc: true
toc_sticky: true
comments: true
permalink: /project/nlp/gpt1/
categories:
  - NLP
  - Paper Review
tags:
  - Language Modeling

use_math: true
last_modified_at: 2020-10-16
---

ì´ë²ˆ ìŠ¤í„°ë”” ìˆœì„œëŠ” GPT-1ì´ë‹¤. ì›ë¬¸ì€ [ì´ê³³](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)ì„ ì°¸ê³ í•˜ì.

ğŸ“’**Keypoint**:
- *generative pre-training*
- *discriminative fine-tuning*
- *task-aware input transformation*

ë³¸ë¬¸ì„ ë³´ë©´ generative pre-training í›„ discriminative fine-tuningì´ë¼ëŠ” ë§ì´ ë‚˜ì˜¨ë‹¤. ì´ê²Œ í”íˆë“¤ ë§í•˜ëŠ” generative modelê³¼ discriminative modelì„ ì¹­í•˜ëŠ” ê±´ê°€ ê¶ê¸ˆí–ˆë‹¤.

***Generative pre-training***: ë³„ ì´ìœ  ì—†ë‹¤. ê·¸ëƒ¥ textë¥¼ generationí•˜ê¸° ë•Œë¬¸ì´ë‹¤. (ê·¼ë° ì–´ëŠ LMì´ë‚˜ generationí•˜ì§€ ì•Šë‚˜ ì‹¶ì€ë°..)
{: .notice--info}

***discriminatvie fine-tuning***: [Universal Language Model Fine-tuning for Text Classification](https://www.aclweb.org/anthology/P18-1031/)ì—ì„œ ì†Œê°œëœ ê²ƒìœ¼ë¡œ, ì¼ì¢…ì˜ fine-tuning ì „ëµì´ë‹¤. ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•´ ê°™ì€ learning rateë¥¼ ì ìš©í•˜ëŠ” ëŒ€ì‹  ê° layerì— ë‹¤ë¥¸ learning rateë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì›ë˜ë¼ë©´ $\theta _t = \theta _{t-1} - \eta \cdot \nabla _\theta J(\theta)$ì™€ ê°™ì€ SGDëŠ”, $\theta ^l _t = \theta ^l _{t-1} - \eta ^l \cdot \nabla _\theta J(\theta)$ì™€ ê°™ì´ ë³€í•˜ê²Œ ëœë‹¤.
{: .notice--info}

<div class="notice--info" markdown="1">

***task-agnostic***: task-agnostic; that is, it can be **universally applied to various downstream NLP tasks** via fine-tuning [MobileBERT: Task-Agnostic Compression of BERT by Progressive Knowledge Transfer](https://openreview.net/forum?id=SJxjVaNKwB).

The question we investigate in this paper is how to effectively use unlabeled data: in a **task-agnostic** or a task-specificway. An example of the former is training models on language model (LM) like objectives on a large unlabeled corpus to learn general representations,as in ELMo (Embeddings from Language Models)(Peters et al., 2018) and BERT (Bidirectional En-coder Representations from Transformers) (Devlinet al., 2019). [To BERT or not to BERT: Comparing task-specific and task-agnostic semi-supervised approaches for sequence tagging](https://www.amazon.science/publications/to-bert-or-not-to-bert-comparing-task-specific-and-task-agnostic-semi-supervised-approaches-for-sequence-tagging)

ì¦‰, taskì— ê´€ë ¨ì—†ì´(agnostic) ì§„í–‰ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. LMì˜ ê²½ìš° ëŒ€í‘œì ì¸ agnosticìœ¼ë¡œ, large corpus unlabeled dataë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

</div>

# 1. Introduction

- ë‹¨ì–´ ë‹¨ìœ„ ì´ìƒì˜ ì •ë³´ë¥¼ unlabeled textì— ì–»ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì€ ë‘ ê°€ì§€ ì´ìœ  ë•Œë¬¸ì— ë§¤ìš° ì–´ë ¤ì›€
    - text representationì—ì„œ transfer learningì„ ìœ„í•´ ê°€ì¥ íš¨ê³¼ì ì¸ optimization objectiveê°€ ë¬´ì—‡ì¸ì§€ ë¶ˆë¶„ëª…í•˜ë‹¤.
        - ELMoê°™ì€ LMë¶€í„°, NMT, discourse coherenceì™€ ê°™ì€ ë‹¤ì–‘í•œ taskë¥¼ í†µí•´ ì„±ê³µì ì¸ representationì„ ì–»ì–´ëƒˆë‹¤.
    - ì´ë ‡ê²Œ ì–»ì€ í‘œí˜„ì— ëŒ€í•´ target taskë¡œ transfer learningì„ ì§„í–‰í•  ë•Œ, ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì— ëŒ€í•œ í•©ì˜(consensus)ê°€ ì—†ë‹¤.
        - ì´ì—ëŠ” (1) ëª¨ë¸ì˜ ì•„í‚¤í…ì³ë¥¼ ë³€í™”ì‹œí‚¤ê±°ë‚˜, íŠ¸ëœìŠ¤í¼ ëŸ¬ë‹ì„ í•  ìˆ˜ ìˆëŠ” ë³´ì¡°ê²©ì˜ ëª©ì í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.
- GPT-1ì€ language understanding taskë¥¼ ìœ„í•œ semi-supervised approachë¥¼ íƒêµ¬í•œë‹¤. 
    - ì´ë¥¼ ìœ„í•´ unsupervised pre-trainingê³¼ supervised fine-tuningì„ í™œìš©
    - ë³¸ ë…¼ë¬¸ì€ unlabeled large corpusì™€ ì§ì ‘ annotateí•œ ë°ì´í„° ì…‹ (target tasks)ë¥¼ í™œìš©í•˜ëŠ” ìƒí™©ì„ ê°€ì •
    - target tasksëŠ” unlabeled corpusì™€ ê°™ì€ domainì— ìˆì§€ ì•Šì•„ë„ ë¨ 
- ë³¸ ì‘ì—…ì€ two-stage training procedureë¥¼ ê°–ëŠ”ë°, 
    - ë¨¼ì € unlabeled dataì— ëŒ€í•´ language modeling objectiveë¥¼ ì‚¬ìš©í•˜ì—¬ initial parameterë¥¼ ì–»ê³ ,
    - ì´í›„ ì´ë¥¼ supervised objectiveì— transferí•œë‹¤.
- GPT-1ì€ transformerë¥¼ ì‚¬ìš©
- ë˜í•œ transfer learningì„ ìœ„í•´ ***traversal-style approaches***ì˜ task-specific input adaptationì„ í™œìš©
    - ì´ëŠ” êµ¬ì¡°í™”ëœ text inputì„ í•˜ë‚˜ì˜ contiguous sequence of tokensë¡œ ë§Œë“œëŠ” ê²ƒ
    - ì•ì„œ ì–¸ê¸‰í–ˆë“¯ fine-tuneì„ íš¨ê³¼ì ì´ê³ , ëª¨ë¸ì—ì„œ ìµœì†Œí•œì˜ ë³€ê²½ì„ ê°€ëŠ¥í•˜ê²Œ í•¨

***Traversal-style Approaches:*** [ì›ë¬¸](https://arxiv.org/pdf/1509.06664.pdf)ê³¼ ê°™ì´ entailmentê³¼ ê°™ì€ ì‘ì—…ì—ì„œ premiseì™€ hypothesisë¥¼ ë¬¶ì–´ ë†“ëŠ” ê²ƒì„ ì˜ë¯¸
{: .notice--info}

# 2. Related Work

## Semi-supervised learning for NLP

- GPT-1ì´ ì†í•˜ëŠ” ë¶„ì•¼ë¡œ, word-level/phrase-levelê¹Œì§€ ë‹¤ì–‘í•˜ê²Œ ì´ìš©ë˜ì–´ì˜´
    - e.g. word2vec, GloVe, Doc2vecv, etc
- ìµœê·¼ë“¤ì–´ unlabeled corporaì„ ì´ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ ìµœê·¼ ë“¤ì–´ ìœ í–‰í•˜ê¸° ì‹œì‘í•¨
- ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì ‘ê·¼ë²•ë“¤ì€ GPT-1ì´ ë†’ì€ ìˆ˜ì¤€ì˜ ì˜ë¯¸ë¥¼ í¬ì°©í•˜ë ¤ëŠ” ê²ƒê³¼ëŠ” ë‹¤ë¥´ê²Œ ì£¼ë¡œ word-levelì—ì„œ transferê°€ ì¼ì–´ë‚¨

## Unsupervised pre-training

- Unsupervised pre-trainingëŠ” supurvised learning objectiveë¥¼ ìˆ˜ì •í•˜ëŠ” ëŒ€ì‹  ì¢‹ì€ ì‹œì‘ì ì„ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•  ë•Œ semi-supervised learningì˜ íŠ¹ìˆ˜í•œ ê²½ìš°
- ì¼ì¢…ì˜ regularization ì—­í• ë¡œ, ì¼ë°˜í™”ì— ë„ì›€ì„ ì£¼ì—ˆì§€ë§Œ, ì§€ê¸ˆì€ neural net í•™ìŠµ ê³¼ì •ì— ë„ì›€ë˜ëŠ” ë°©ë²•ë¡ ì´ ê°œë°œ
- GPT-1ì€ natural language inference, paraphrase dectection, story completionê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë²”ìœ„ì—ì„œì˜ íš¨ê³¼ê°€ ìˆìŒ
- ELMoë‚˜ ë‹¤ë¥¸ ì—°êµ¬ë“¤ì€ pre-trained LMì´ë‚˜ machine translation modelë¡œ ë¶€í„° hidden representationì„ ì–»ê³ , ì´ë¥¼ ë³´ì¡°ì ì¸ feature(auxiliary feature)ë¡œ ì‚¬ìš©í•¨
    - ì´ëŠ” ì¶”ê°€ì ì¸ parameterë¥¼ í•™ìŠµí•´ì•¼ í•¨
    - ê·¸ëŸ¬ë‚˜ GPT-1ì€ ëª¨ë¸ì˜ ìµœì†Œí•œì˜ ë³€ê²½ë§Œì„ ìš”êµ¬

ì•„ë˜ì˜ termì€ ì‚¬ëŒë§ˆë‹¤ ì•½ê°„ì”© ì“°ëŠ” ë‹¨ì–´ê°€ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ê°œì¸ì ì¸ ì˜ê²¬ìœ¼ë¡œëŠ” ML/DLì— ë”°ë¼ì„œ ì´ëŸ¬í•œ ê²½í–¥ì„ ë³´ì´ëŠ” ê²ƒ ê°™ë‹¤. ë”°ë¼ì„œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ, ì„œì /ë…¼ë¬¸ê°™ì€ ì˜¤í”¼ì…œí•œ ë¬¸ì„œë¥¼ ìµœëŒ€í•œ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ë‹¤.
{: .notice--dange}

***Supervised Task-specific Pre-training:*** task Aë¥¼ ì§„í–‰í•˜ë ¤ê³  í•˜ê³ , ì´ì—ëŠ” label dataê°€ ì œí•œë˜ì–´ ìˆë‹¤ê³  í•˜ì. ê·¸ëŸ¬ë‚˜ ì´ ì‘ì—…ì€ task Bë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ì´ì—ëŠ” ë°ì´í„°ê°€ ë§ë‹¤. ì´ëŸ¬í•œ ê²½ìš°ì— ìš°ë¦¬ëŠ” task Bë¥¼ í†µí•´ ì¢‹ì€ representationì„ ë¨¼ì € ì–»ê³ (pre-trained), ê·¸ í›„ì— task Aë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì¼ ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì¼€ì´ìŠ¤ë¥¼ supervised Task-specific Pre-trainingì´ë¼ í•œë‹¤. ì´ ê²½ìš° ìš°ë¦¬ëŠ” pre-trained vectorë¥¼ ê³ ì •í•  ìˆ˜ë„ ìˆê³ , tuneí•  ìˆ˜ë„ ìˆë‹¤. ë˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” ë‘ ê°œì˜ objeciveë¥¼ jointlyí•˜ê²Œ ë°°ìš°ëŠ” í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.
{: .notice--info}

***Semi-supervised Learning:*** Semi-supervised learningì—ì„œ ë¨¸ì‹ ì€ label/unlabel ë°ì´í„° ë‘˜ì„ ëª¨ë‘ í™œìš©í•˜ê²Œ ëœë‹¤. label ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œ ì´í›„ì— unlabel ì¤‘ í™•ì‹¤í•˜ê²Œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ì„ ì •í•˜ê³ , ë‹¤ì‹œ ì´ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµ, ë¶„ë¥˜ë¥¼ ë°˜ë³µí•˜ëŠ” ì‘ì—…ì´ë‹¤.
{:. notice--info}

***Unsupervised Pre-training:*** ì¼ë°˜ì ì¸ ê²½ìš°ì—ëŠ” ìœ„ì™€ ê°™ì´ ì¶©ë¶„í•œ labeled dataë¥¼ ê°–ëŠ” auxiliary task (i.e. task B)ê°€ ì—†ë‹¤ (í˜¹ì€ bootstrap í• ìˆ˜ë„ ìˆê³ ). ì´ ê²½ìš°ì—ëŠ” unsupervised methodë¥¼ ì‚¬ìš©í•œë‹¤. word vectorë¥¼ ë§Œë“œëŠ” ë°©ë²•ì€ ê¸°ë³¸ì ìœ¼ë¡œ supervised learningì„ ì´ìš©í•˜ëŠ” ê²ƒì´ì§€ë§Œ, ìš°ë¦¬ê°€ í•˜ê³  ì‹¶ì€ taskì— ëŒ€í•œ supervision dataë¥¼ ì“°ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, raw textë¡œë¶€í„° ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” word2vecê³¼ ê°™ì´ ë¹„ìŠ·í•œ ë‹¨ì–´ì˜ vectorë¥¼ ì–»ëŠ” ê²ƒì´ê³ , **distribtuional hypothesis**ë¥¼ ê°€ì •í•œë‹¤. ì´ë¡œ ì¸í•œ ì£¼ìš” ì¥ì ìœ¼ë¡œëŠ” supervised taskì— ë“±ì¥í•˜ì§€ ì•Šì€ ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.
{: .notice--info}

ê·¸ëŸ¬ë‚˜ ì´ ì •ì˜ëŠ” ë‹¤ì‹œê¸ˆ ë‚˜ì—ê²Œ **ê·¸ë ‡ë‹¤ë©´ self-supervised learningê³¼ unsupervised pre-trainingì´ ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€?**ë¼ëŠ” ì˜ë¬¸ì„ ë‚¨ê¸´ë‹¤.

***Self-supervised learning:*** Pre-trainingê³¼ ê´€ë ¨ëœ ê°œë…ìœ¼ë¡œ, ì‚¬ì „í•™ìŠµì„ í†µí•´ downtream taskì—ì„œ ì´ë“ì„ ë³´ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” labelì—†ëŠ”(unsupervised) ë°ì´í„°ë¡œ supervised learningì„ í•˜ëŠ” ê²ƒì´ë‹¤. ì–€ ë¥´ì¿¤ì˜ ì„¤ëª…ì„ ë³´ì.
![Yann LeCun's mention on self-supervised learning](https://user-images.githubusercontent.com/47516855/97088766-0d487c80-166e-11eb-9c63-06734b4b0c7d.png){: .align-center}
{: .notice--info}

***Semi-supervised learning:*** ì•ì„œ semi supervised learningì´ label/unlabel ë‘˜ ë‹¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë¼ ë§í•˜ì˜€ë‹¤. ì¦‰, unlabeled dataë¥¼ í†µí•´ LMì„ í•™ìŠµí•˜ê³ , ì•½ê°„ì˜ ë°ì´í„°ë¥¼ í†µí•´ fine-tuningì„ í•˜ëŠ” approachë¥¼ semi-supervised learningì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
{: .notice--info}

ì´ëŸ¬í•œ ë°©ë²•ë¡ ë“¤ì€ ëª¨ë‘ **pre-trained model**ì„ ì–»ê¸° ìœ„í•¨ì´ë¼ëŠ” ê³µí†µì ì´ ìˆë‹¤.

## Auxiliary training objectives

- ***Auxiliary training objectives***ëŠ” ë³´ì¡°ì ì¸ unsupurvised training objectiveë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì€ semi-supervised learningì˜ ì¼ì¢…ìœ¼ë¡œ ë³´ë©´ ëœë‹¤
    - ì¦‰, ì›ë˜ í•™ìŠµ ëª©ì ê³¼ëŠ” ë³„ê°œë¡œ ëª©ì í•¨ìˆ˜ì— ì¶”ê°€í•˜ëŠ” ê²ƒì„ ì˜ë¯¸
- ì‹¤í—˜ì—ì„œë„ ì‚¬ìš©í•˜ê¸´ í–ˆì§€ë§Œ, ì´ì™€ ë³„ê°œë¡œ unsupervised pre-trainingì´ target taskì™€ ì—°ê´€ìˆëŠ” ì–¸ì–´ì  ìš”ì†Œ(linguistic aspect)ë¥¼ ë°°ìš°ëŠ” ê²ƒì„ í™•ì¸

***Auxiliary training objectives:*** BERTë¥¼ ì•„ì§ ë°°ìš°ì§„ ì•Šì•˜ì§€ë§Œ BERTì˜ ê²½ìš°ì—ëŠ” next sentenceë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜ mask ì”Œìš´ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´ ê²½ìš° ì´ ë‘˜ì˜ lossë¥¼ í•©ì¹˜ê²Œ ë˜ëŠ”ë°, GPT-1ì—ì„œ ë§í•˜ëŠ” auxiliraryëŠ” ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ ì‹¤ì œ ëª©ì ê³¼ëŠ” ë¬´ê´€í•˜ì§€ë§Œ, lossì— ì¶”ê°€í•˜ì—¬ ì´ë“ ë³´ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
{: .notice--info}


# 3. Framework

- ì•ì„œ ë§í–ˆë“¯ LMì„ ë°°ìš°ê³ , fine-tuneí•˜ëŠ” í˜•íƒœë¡œ ì§„í–‰

## 3.1 Unsupervised pre-training

- Unsupervised corpus tokens $\mathcal U = {u _1, ..., u _n}$ì— ëŒ€í•´, ì¼ë°˜ì ì¸ LM ojbectiveë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒì˜ likelihoodë¥¼ ìµœëŒ€í™”

$$
\begin{align}
L_1(\mathcal U) = \sum _i log P (u _i \lvert u _{i-k}, ..., u _{i-1}; \Theta)
\end{align}
$$

- **këŠ” context window**, conditional probability $P$ëŠ” parameter $\Theta$ë¥¼ ì´ìš©í•œ neural netì„

***context window:*** word2vecê°™ì€ ê²½ìš° context vectorë¥¼ ì„¤ì •í•˜ê³ , center word ì „í›„ë¡œ window sizeë¥¼ conditionìœ¼ë¡œ ì£¼ê²Œëœë‹¤. ì§€ê¸ˆì€ context windowê°€ ì´ì „ ì‹œì  ë°–ì— ì—†ìœ¼ë¯€ë¡œ Auto-regressiveí•˜ê²Œ ë„ëŠ” ê²ƒìœ¼ë¡œ ë³´ì´ëŠ”ë°, ì´ ê²½ìš° context windowê°€ dynamicí•˜ê²Œ ë„ëŠ”ì§€ í™•ì¸í•  í•„ìš”ê°€ ìˆì–´ë³´ì¸ë‹¤.
{: .notice--danger}

- ì‹¤í—˜ì—ì„œëŠ” ***multi-layer transformer decoder***ë¥¼ ì‚¬ìš©
- input context token - multi-layer transformer decoder - position-wise feedforward ìˆœì„œë¡œ ì§„í–‰

$$
\begin{align}
& h _0 = UW _e + W _p \\
& h _l = \textrm{transformer_block} (h _{l-1}) \forall \in [1, n] \\
& p(U) = \textrm{softmax}(h _n W _e ^T) \\
\end{align}
$$

- $ U = (u _{-k}, ..., u _{-1})$ì´ê³ , $n$ì€ layerì˜ ìˆ˜, $W _e$ëŠ” token embedding matrix, $W _p$ëŠ” position embedding matrix

***multi-layer transformer decoder:*** Citationì´ ê±¸ë¦° [ë…¼ë¬¸](https://arxiv.org/pdf/1509.06664.pdf)ì„ í™•ì¸í•´ë³´ì•˜ëŠ”ë°, *local attention*ê³¼ *memory-compressed attention*ì´ë¼ëŠ” ê°œë…ì„ ì†Œê°œí•˜ê³  ìˆë‹¤. GPT-1ì—ì„œë„ ì´ëŸ¬í•œ ê°œë…ì„ ì‚¬ìš©í•˜ëŠ”ì§€ëŠ” ì˜ë¬¸.
{: .notice--danger}

## 3.2 Supervised fine-tuning

- Eq. 1ì˜ objectiveë¥¼ ë”°ë¼ trainingí•œ í›„ supervised target taskì— transferí•˜ë©´ ë¨
- ì—¬ê¸°ì„  labeled dataset $\mathcal C$ë¥¼ ê°€ì •
    - $x^1, ..., x^m$ê³¼ ê°™ì€ sequence of input tokenê³¼ label $y$ë¡œ ì´ë£¨ì–´ì§
- inputì€ pre-trained modelì„ ì§€ë‚˜ ë§ˆì§€ë§‰ transformer blockì—ì„œ $h^m _l$ì„ ì–»ê³ , linear output layer $W _y$ë¥¼ í†µí•´ $y$ë¥¼ ì˜ˆì¸¡

$$
P(y \rvert x^1, ..., x^m) = \textrm{softmax}(h^m _l W_y)
$$

- ì´ëŠ” ë‹¤ìŒ objectiveë¥¼ maximizeí•˜ê²Œ í•œë‹¤

$$
L _2 (\mathcal C) = \sum _{(x, y)} log P(y \rvert x^1, ..., x^m)
$$

- ì¶”ê°€ì ìœ¼ë¡œ, LMì„ auxiliary objectiveë¥¼ fine-tuningì— í¬í•¨í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë˜ëŠ” ê²ƒì„ ë°œê²¬
    - supervised modelì˜ generalizationì„ í–¥ìƒ
    - convergenceë¥¼ ë¹ ë¥´ê²Œ ë„ì™€ì¤Œ
- êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ objectiveë¥¼ optimize (with weight $\lambda$)

$$
L_3(\mathcal C) = L _2(\mathcal C) + \lambda * L _1(\mathcal C)
$$

-fine-tuningë‹¨ê³„ì—ì„œ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµí•  parameterëŠ” $W_y$ì™€ delimiter tokenì„ ìœ„í•œ embeddingë°–ì— ì—†ìŒ

![figure 1](https://user-images.githubusercontent.com/47516855/96349181-c21bf000-10e8-11eb-9dd1-2bfa44badfc0.png){: width="800"}{: .align-center}

## 3.3 Task-specific input transformations

- ì–´ë–¤ tasksì—ì„  ìœ„ì—ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´ ì§ì ‘ì ìœ¼ë¡œ fine-tuneí•  ìˆ˜ ìˆìŒ
- ê·¸ëŸ¬ë‚˜ QA/textual entailmentê°™ì€ structured inputì€ delimiter tokenì„ í†µí•´ sequenceë¥¼ í•©ì¹¨ (i.e. traversal-style approach)

# 4. Experiments

## 4.1 Setup

- 12ê°œì˜ transformer layers
- 12ê°œì˜ head (í•˜ë‚˜ ë‹¹ 64dim)
- position-wise FFNì˜ sizeëŠ” 3072
- Adam optimization
    - 0ë¶€í„° 2000ê¹Œì§€ max lr: 2.5e-4
    - ê·¸ í›„ ***cosine schedule***ì„ ì´ìš©í•˜ì—¬ 0ê¹Œì§€ annealing
- 100 epochs, 64 mini-batch
- 512ê°œì˜ contiguous tokens
- weight init: $N(0, 0.02)$
    - layer normì´ ë§ì´ ì‚¬ìš©ë˜ì—ˆê¸° ë•Œë¬¸ì— ì´ê±°ë¡œë„ ì¶©ë¶„í•¨
- BPE: 40000 tokens
- dropout(p=0.1)
- ***L2 regularization ($w=0.01$)***
- GELU activation

***cosine schedule (Cosine annealing)***: cosine annealingì€ learning rate scheduleì˜ í•œ ì¢…ë¥˜ë¡œ, cosine í•¨ìˆ˜ì²˜ëŸ¼ í° learning rateë¡œ ì‹œì‘í•˜ì—¬ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§€ê³  ë‹¤ì‹œ ì˜¬ë¼ê°€ëŠ” ê²ƒì„ ë°˜ë³µí•œë‹¤. ì´ì²˜ëŸ¼ learning rateë¥¼ ë†’ì´ëŠ”(resetting) í–‰ìœ„ëŠ” í•™ìŠµì„ ì¬ì‹œì‘í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì—­í• ì„ í•˜ë©°, ì´ ì¬ì‹œì‘ì—ì„œì˜ ì´ˆê¸°ê°’ì€ ì´ë¯¸ í•™ìŠµì„ í•œë²ˆ í•œ ì¢‹ì€ weightê°€ ëœë‹¤. ì´ëŸ¬í•œ ê²ƒì„ *warm start*ë¼ê³  ë¶€ë¥´ë©°, *cold start*ê°€ ëœë¤í•œ ì§€ì ì—ì„œ í•™ìŠµì„ ì‹œì‘í•˜ëŠ” ê²ƒê³¼ëŠ” ëŒ€ì¡°ì ì´ë‹¤. ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
$\eta _t = \eta ^i _{min} + \cfrac{1}{2} (\eta ^i _{max} - \eta ^i _{min})(1 + cos(\cfrac {T _{cur}} {T _i} \pi)) $ ì—¬ê¸°ì„œ $\eta ^i _{max}$ì™€ $\eta ^i _{min}$ëŠ” ê° ê° learning rateì˜ ë²”ìœ„ì´ê³ , $T _{cur}$ì€ ì´ì „ ì¬ì‹œì‘ìœ¼ë¡œë¶€í„° ì–¼ë§ˆë‚˜ epochì´ ëŒì•˜ëŠ”ì§€ ì²´í¬í•œë‹¤.
{: .notice--info}

***L2 regularization ($w=0.01$):*** [ì›ë¬¸](https://openreview.net/forum?id=rk6qdGgCZ)/[Revision versionìœ¼ë¡œ ë³´ì´ëŠ” ë…¼ë¬¸](https://arxiv.org/abs/1711.05101)ì„ ë³´ë©´, SGDì™€ëŠ” ë‹¬ë¦¬ Adamì—ì„œ L2 regularizationê³¼ weight decayëŠ” ì„œë¡œ ë‹¤ë¥¸ ê²ƒì´ë¼ê³  í•œë‹¤. ë‹¤ìŒì„ ì°¸ê³ í•´ë³´ì. [AdamWì— ëŒ€í•´ ì•Œì•„ë³´ì! Decoupled weight decay regularization ë…¼ë¬¸ ë¦¬ë·°(1)](https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.html). OpenAi
{: .notice--info}

